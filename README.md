# FreeMultimodal
A multimodal system capable of seeing, hearing, speaking, and reasoning about a plethora of information from multiple input streams.

The current system is able to read images based on the weights from YOLOV7 and use OpenAI LLMs like ChatGPT (i.e., GPT-3.5 and GPT-4) to have a conversation about the image. 

To do:

1- Incorporate LangChain for reasoning about documentation from different file types (DOCX, PDF, CSV, XLSX, JSON, etc.).
2- Create a video to showcase capabilities.
3- Deploy the model
